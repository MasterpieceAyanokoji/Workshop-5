# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Тарасов Денис Дмитриевич
- ФО-220007
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Обучиться использованию Ml агентов, определить какие параметры и как влияют на обучение модели, а также определить в каких случаях проще использовать ML-агент, а не писать программную реализацию решения

Ход работы:

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

В ходе анализа кода выяснилось. что текущую взаимосвязь между Agent и Target, а в этом и суть коэффицента корреляции, выражает переменная "distanceToTarget"

Чем меньше коээфицент корреляции, тем лучше и быстрее обучится модель, так как в данном случае он является границей, определяющей вознаграждение. Однако, уменьшение данного параметра может сильно увеличить время обучения модели, при выборе стоит учитывать и этот параметр.


## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

визуализация динамики процесса обучения в зависимости от изменяемой величины будет произведена посредством TensorBoard. Для каждой попытки обучения будет предпринято 100 тысяч шагов

1) time_horizon - количество шагов, которое должен сделать агент, прежде чем он получит награду. Если число будет слишком маленьким, то это делает обучение не точным или вовсе невозможным, если достижение цели требует большое количество действий от агента.

Значение 32:

![Alt text](https://sun9-33.userapi.com/impg/AmXYR4OSsClLcprB_uewJnirPuPYdsqZkO4bkw/XZfiqy0n7nA.jpg?size=835x802&quality=96&sign=89a2c7a84ceea973797bcfea2ca203aa&type=album "Environment, time_horizon = 32")

![Alt text](https://sun3-21.userapi.com/impg/uku__DgpRTMKnKvbgLGI-SDtiULMZhnlT1PfCw/KjYvm1FtcyQ.jpg?size=1199x723&quality=96&sign=49b6f9c22028fa824e46f9c58c3f4efd&type=album "Policy, time_horizon = 32")

---

Значение 2048:

![Alt text](https://sun9-17.userapi.com/impg/mI5XZipTgLlwp3RejEY295ozoB4d-5nEBZuB5A/x6if7xXrXWk.jpg?size=850x809&quality=96&sign=6850d30b9a8869c94a2629e4b08901a1&type=album "Environment, time_horizon = 2048")

![Alt text](https://sun9-65.userapi.com/impg/kQ4q9EcvV5TV_VfTDMk0EijxZLV83k6Ivgbhxw/ZoSuw_DVek8.jpg?size=1187x731&quality=96&sign=30d333b961cd7adcafbcd55e0674344e&type=album "Policy, time_horizon = 2048")

На графиках видно, что при низком значении награждение агента происходит достаточно стабильно, так как для текущей задачи ему не нужно сделать многого, чего нельзя сказать про выскокие значения. При высоких значениях, агент успевает наделать много лишних действий, что приводит к ухудшению его результата.

2) extrinsic -> strength - коэффициент кратности награды. Чем он больше, тем более высокая награда выдаётся агенту

Значение 0.25:

![Alt text](https://sun9-67.userapi.com/impg/Cwcpsvk2mRQMF98jp3GJAxjE1gdxuj5CqQeJzw/OZutkyA-a6Y.jpg?size=840x805&quality=96&sign=1e8facbf9f883d9e0a15f0d4ca89a1e1&type=album "Environment, extrinsic -> strength = 0.25")

![Alt text](https://sun9-52.userapi.com/impg/0yEddNJCVFohgQYX0gwNOVA94ORADEqQSyJPqA/9x49yj2zpKc.jpg?size=1211x719&quality=96&sign=9af908ab3b62cf1d4c8de538fd4e6574&type=album "Policy, extrinsic -> strength = 0.25")

---

Значение 10.0:

![Alt text](https://sun9-78.userapi.com/impg/Oaa8Ds1mHFC_VU_rCpjwbsxuvnTiBU8bI0OjQA/_zUNwounJDo.jpg?size=840x791&quality=96&sign=e645c5be802ab4831d0802fd3dd3693a&type=album "Environment, extrinsic -> strength = 10.0")

![Alt text](https://sun9-45.userapi.com/impg/5QICux2yoNfs4BMC4KVb9QdjbpefLdsARUNieg/d4_-suDiLtI.jpg?size=1199x725&quality=96&sign=039e46a39f3ee433b5cf9d9f29965783&type=album "Policy, extrinsic -> strength = 10.0")

При внутреннем сигнале награды = 1, внешняя награда строго равна коэффициенту кратности, но слишком высокие значения внешней награды приводят к дистабилизации процесса обучения и необходимо подстраивать это значение в зависимости от внутреннего сигнала награждения. 

3) extrinsic -> gamma - коэффициент скидки будущих награждений.
Этот коэффициент определяет, насколько агенту будут важны будущие награды.
Чем этот коэффициент больше, тем большую направленность будут иметь действия агента на получение награды в будущем, а не в настоящем. Принимает значения находящиеся в интервале (0; 1)

Значение 0.01:

![Alt text](https://sun9-58.userapi.com/impg/ULfv8UXh4YkMLG5t85si6ou8bFwd5j90Livq0g/HrS54sJ9Isk.jpg?size=825x806&quality=96&sign=80a705907c472a53c0079a6c87d304eb&type=album "Environment, extrinsic -> gamma = 0.01")

![Alt text](https://sun9-6.userapi.com/impg/7bHpMRaju6KSWYlw3MwIEw6gIcU5ILjLt1ff1w/6KyAcueR6DQ.jpg?size=1190x717&quality=96&sign=07e9fa29ded877800c9680b2cd4ac63c&type=album "Policy, extrinsic -> gamma = 0.01")

---

Значение 0.99:

![Alt text](https://sun9-25.userapi.com/impg/pW8Db3LeMyL2UQXX-QQ3WDAhen8K3PCY4eQPhg/EDdDyBbq9LE.jpg?size=820x805&quality=96&sign=21d9f4cf1815b8a6109295197454b89e&type=album "Environment, extrinsic -> gamma = 0.99")

![Alt text](https://sun9-68.userapi.com/impg/PwxYlU8LhMN_VUv22gH1c5qZyo95JcgrnenCzw/PA3Oj2pY5s4.jpg?size=1221x722&quality=96&sign=448e9a90e8f0557f4375eb75071d4bc5&type=album "Policy, extrinsic -> gamma = 0.99")

При высоких значениях данного параметра мы можем видеть хоть слегка и запоздавшее (с 20000 шага), но стабильное значение награды равное 1, что демонстрирует работу агента на будущее. 
Напротив, маленькое значение вызвало нарастающую награду фактически с самого начала обучения, так как агент действовал сразу.


## Задание 3
### Приведите примеры, для каких игровых задач и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

Использование ML-агентов обычно проще в случаях, когда задача сложна или когда требуется обучение на большом объеме данных. ML-агенты могут эффективно обучаться на основе опыта и данных, что может быть сложно или трудоемко реализовать вручную. Агент, который использовался во 2 примере может быть полезен в разработке многих жанров, к примеру, где npc должны следовать за игроком или другим предметом, где существует преследование определенной точки, как например hello neighbor, игра в которой тебя преследует npc в виде соседа.

Агент, который следует по определённому маршруту от точки А к точке Б может быть использован для разработки npc, которые будут находить способ выходить из ситуации, такой агент хорошо бы реализовался в гоночных играх, где есть старт и финиш, к примеру игра Need For Speed.

ML-агенты могут быть применены в играх, когда требуется создание умных и адаптивных противников или союзников, способных адаптироваться к действиям игроков и создавать реалистичное игровое окружение. Также они могут быть использованы для обучения игроков в сложных стратегических играх, таких как шахматы, где необходимо анализировать большое количество данных и принимать сложные стратегические решения. Либо же, если конечный алгоритм программной реализации будет слишком сложным, громоздским и трудно-читаемым. Напротив, если задача очень простая, а игровой мир является статичным, то в использовании ML-Agent нет смысла и он только нагрузит игру

## Выводы
Я ознакомился с некоторыми ML агентами и определил какие параметры вляют на обучение модели, также опытным путем выяснил, что в некторых ситуациях легче использовать Ml агентов, чем использовать обычный код.
Цели лабораторной работы были достигнуты.
## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
